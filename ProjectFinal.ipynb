{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_HOME']=\"/opt/cloudera/parcels/SPARK2/lib/spark2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf().\\\n",
    "    setAppName('sentiment-analysis-Project4').\\\n",
    "    setMaster('local[2]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, HiveContext\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-11-30 11:21:56--  https://github.com/majedab/IST718Project/blob/master/dfFineFood_2.csv?raw=true\n",
      "Resolving github.com (github.com)... 192.30.253.113, 192.30.253.112\n",
      "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github.com/majedab/IST718Project/raw/master/dfFineFood_2.csv [following]\n",
      "--2016-11-30 11:21:56--  https://github.com/majedab/IST718Project/raw/master/dfFineFood_2.csv\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://media.githubusercontent.com/media/majedab/IST718Project/master/dfFineFood_2.csv [following]\n",
      "--2016-11-30 11:21:56--  https://media.githubusercontent.com/media/majedab/IST718Project/master/dfFineFood_2.csv\n",
      "Resolving media.githubusercontent.com (media.githubusercontent.com)... 151.101.116.133\n",
      "Connecting to media.githubusercontent.com (media.githubusercontent.com)|151.101.116.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 277639337 (265M) [text/plain]\n",
      "Saving to: ‘dfFineFood_copy.csv’\n",
      "\n",
      "100%[======================================>] 277,639,337  180MB/s   in 1.5s   \n",
      "\n",
      "2016-11-30 11:22:04 (180 MB/s) - ‘dfFineFood_copy.csv’ saved [277639337/277639337]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/majedab/IST718Project/blob/master/dfFineFood_2.csv?raw=true -O dfFineFood_copy.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fineFood_df = sqlContext.read\\\n",
    "    .format(\"com.databricks.spark.csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"nullValue\", 'NA')\\\n",
    "    .load(\"file:///home/maalbarr/Project/dfFineFood_copy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-11-30 00:03:23--  https://github.com/daniel-acuna/python_data_science_intro/blob/master/data/sentiments.parquet.zip?raw=true\n",
      "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\n",
      "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github.com/daniel-acuna/python_data_science_intro/raw/master/data/sentiments.parquet.zip [following]\n",
      "--2016-11-30 00:03:23--  https://github.com/daniel-acuna/python_data_science_intro/raw/master/data/sentiments.parquet.zip\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/daniel-acuna/python_data_science_intro/master/data/sentiments.parquet.zip [following]\n",
      "--2016-11-30 00:03:23--  https://raw.githubusercontent.com/daniel-acuna/python_data_science_intro/master/data/sentiments.parquet.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.116.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.116.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 38387 (37K) [application/zip]\n",
      "Saving to: ‘sentiments.parquet.zip’\n",
      "\n",
      "100%[======================================>] 38,387      --.-K/s   in 0.01s   \n",
      "\n",
      "2016-11-30 00:03:23 (3.71 MB/s) - ‘sentiments.parquet.zip’ saved [38387/38387]\n",
      "\n",
      "Archive:  sentiments.parquet.zip\n",
      "   creating: sentiments.parquet/\n",
      "  inflating: sentiments.parquet/.part-r-00000-e719650f-4cd0-4bf6-b325-d485724c78e8.gz.parquet.crc  \n",
      "  inflating: sentiments.parquet/.part-r-00001-e719650f-4cd0-4bf6-b325-d485724c78e8.gz.parquet.crc  \n",
      "  inflating: sentiments.parquet/.part-r-00002-e719650f-4cd0-4bf6-b325-d485724c78e8.gz.parquet.crc  \n",
      "  inflating: sentiments.parquet/.part-r-00003-e719650f-4cd0-4bf6-b325-d485724c78e8.gz.parquet.crc  \n",
      "  inflating: sentiments.parquet/.part-r-00004-e719650f-4cd0-4bf6-b325-d485724c78e8.gz.parquet.crc  \n",
      "  inflating: sentiments.parquet/.part-r-00005-e719650f-4cd0-4bf6-b325-d485724c78e8.gz.parquet.crc  \n",
      "  inflating: sentiments.parquet/.part-r-00006-e719650f-4cd0-4bf6-b325-d485724c78e8.gz.parquet.crc  \n",
      "  inflating: sentiments.parquet/.part-r-00007-e719650f-4cd0-4bf6-b325-d485724c78e8.gz.parquet.crc  \n",
      "  inflating: sentiments.parquet/_common_metadata  \n",
      "  inflating: sentiments.parquet/_metadata  \n",
      " extracting: sentiments.parquet/_SUCCESS  \n",
      "  inflating: sentiments.parquet/part-r-00000-e719650f-4cd0-4bf6-b325-d485724c78e8.gz.parquet  \n",
      "  inflating: sentiments.parquet/part-r-00001-e719650f-4cd0-4bf6-b325-d485724c78e8.gz.parquet  \n",
      "  inflating: sentiments.parquet/part-r-00002-e719650f-4cd0-4bf6-b325-d485724c78e8.gz.parquet  \n",
      "  inflating: sentiments.parquet/part-r-00003-e719650f-4cd0-4bf6-b325-d485724c78e8.gz.parquet  \n",
      "  inflating: sentiments.parquet/part-r-00004-e719650f-4cd0-4bf6-b325-d485724c78e8.gz.parquet  \n",
      "  inflating: sentiments.parquet/part-r-00005-e719650f-4cd0-4bf6-b325-d485724c78e8.gz.parquet  \n",
      "  inflating: sentiments.parquet/part-r-00006-e719650f-4cd0-4bf6-b325-d485724c78e8.gz.parquet  \n",
      "  inflating: sentiments.parquet/part-r-00007-e719650f-4cd0-4bf6-b325-d485724c78e8.gz.parquet  \n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/daniel-acuna/python_data_science_intro/blob/master/data/sentiments.parquet.zip?raw=true -O sentiments.parquet.zip && unzip sentiments.parquet.zip && rm sentiments.parquet.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiments_df = sqlContext.read.load(\"file:///home/maalbarr/Project/sentiments.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "stop_words = requests.get('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words').text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------+------------+-----+--------------------+--------------------+-------------------+\n",
      "| ID|   Time|IsWeekday|IsHolidayUSA|Score|             Summary|                Text|HelpfulnessFraction|\n",
      "+---+-------+---------+------------+-----+--------------------+--------------------+-------------------+\n",
      "|  0|10/8/99|        1|           0|    1|every book is edu...|this witty little...|                  0|\n",
      "+---+-------+---------+------------+-----+--------------------+--------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fineFood_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(ID=0, Time='10/8/99', IsWeekday=1, IsHolidayUSA=0, Score=1, Summary='every book is educational', Text=\"this witty little book makes my son laugh at loud. i recite it in the car as we're driving along and he always can sing the refrain. he's learned about whales, india, drooping roses:  i love all the new words this book  introduces and the silliness of it all.  this is a classic book i am  willing to bet my son will still be able to recite from memory when he is  in college\", HelpfulnessFraction='0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fineFood_df.where(fn.col('Score') == 1).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "tokenizer = RegexTokenizer().setGaps(False)\\\n",
    "  .setPattern(\"\\\\p{L}+\")\\\n",
    "  .setInputCol(\"Text\")\\\n",
    "  .setOutputCol(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "sw_filter = StopWordsRemover()\\\n",
    "  .setStopWords(stop_words)\\\n",
    "  .setCaseSensitive(False)\\\n",
    "  .setInputCol(\"words\")\\\n",
    "  .setOutputCol(\"filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# we will remove words that appear in 5 docs or less\n",
    "cv = CountVectorizer(minTF=1., minDF=5., vocabSize=2**17)\\\n",
    "  .setInputCol(\"filtered\")\\\n",
    "  .setOutputCol(\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we now create a pipelined transformer\n",
    "cv_pipeline = Pipeline(stages=[tokenizer, sw_filter, cv]).fit(fineFood_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---------+------------+-----+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "| ID|    Time|IsWeekday|IsHolidayUSA|Score|             Summary|                Text|HelpfulnessFraction|               words|            filtered|                  tf|\n",
      "+---+--------+---------+------------+-----+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "|  0| 10/8/99|        1|           0|    1|every book is edu...|this witty little...|                  0|[this, witty, lit...|[witty, little, b...|(36579,[2,12,20,6...|\n",
      "|  1|10/25/99|        1|           0|    1|this whole series...|i can remember se...|                  1|[i, can, remember...|[remember, seeing...|(36579,[25,35,37,...|\n",
      "|  2| 12/2/99|        1|           0|    1|entertainingl funny!|beetlejuice is a ...|                  0|[beetlejuice, is,...|[beetlejuice, wri...|(36579,[142,423,1...|\n",
      "|  3| 12/6/99|        1|           0|    1|a modern day fair...|a twist of rumple...|                0.5|[a, twist, of, ru...|[twist, rumplesti...|(36579,[2,581,917...|\n",
      "|  4| 12/6/99|        1|           0|    1|a modern day fair...|a twist of rumple...|                0.5|[a, twist, of, ru...|[twist, rumplesti...|(36579,[2,581,917...|\n",
      "+---+--------+---------+------------+-----+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now we can make the transformation between the raw text and the counts\n",
    "cv_pipeline.transform(fineFood_df).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "idf = IDF().\\\n",
    "    setInputCol('tf').\\\n",
    "    setOutputCol('tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idf_pipeline = Pipeline(stages=[cv_pipeline, idf]).fit(fineFood_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_df, validation_df, testing_df = fineFood_df.randomSplit([0.6, 0.3, 0.1], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[340899, 170355, 57090]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[training_df.count(), validation_df.count(), testing_df.count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambda_par = 0.02\n",
    "alpha_par = 0.3\n",
    "en_lr = LogisticRegression().\\\n",
    "        setLabelCol('Score').\\\n",
    "        setFeaturesCol('tfidf').\\\n",
    "        setRegParam(lambda_par).\\\n",
    "        setMaxIter(100).\\\n",
    "        setElasticNetParam(alpha_par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_lr_estimator = Pipeline(stages=[idf_pipeline, en_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid = ParamGridBuilder().\\\n",
    "    addGrid(en_lr.regParam, [0., 0.01, 0.02]).\\\n",
    "    addGrid(en_lr.elasticNetParam, [0., 0.2, 0.4]).\\\n",
    "    build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Param(parent='LogisticRegression_477f927862dad5f58878', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,\n",
       "  Param(parent='LogisticRegression_477f927862dad5f58878', name='regParam', doc='regularization parameter (>= 0).'): 0.0},\n",
       " {Param(parent='LogisticRegression_477f927862dad5f58878', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,\n",
       "  Param(parent='LogisticRegression_477f927862dad5f58878', name='regParam', doc='regularization parameter (>= 0).'): 0.01},\n",
       " {Param(parent='LogisticRegression_477f927862dad5f58878', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,\n",
       "  Param(parent='LogisticRegression_477f927862dad5f58878', name='regParam', doc='regularization parameter (>= 0).'): 0.02},\n",
       " {Param(parent='LogisticRegression_477f927862dad5f58878', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.2,\n",
       "  Param(parent='LogisticRegression_477f927862dad5f58878', name='regParam', doc='regularization parameter (>= 0).'): 0.0},\n",
       " {Param(parent='LogisticRegression_477f927862dad5f58878', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.2,\n",
       "  Param(parent='LogisticRegression_477f927862dad5f58878', name='regParam', doc='regularization parameter (>= 0).'): 0.01},\n",
       " {Param(parent='LogisticRegression_477f927862dad5f58878', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.2,\n",
       "  Param(parent='LogisticRegression_477f927862dad5f58878', name='regParam', doc='regularization parameter (>= 0).'): 0.02},\n",
       " {Param(parent='LogisticRegression_477f927862dad5f58878', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.4,\n",
       "  Param(parent='LogisticRegression_477f927862dad5f58878', name='regParam', doc='regularization parameter (>= 0).'): 0.0},\n",
       " {Param(parent='LogisticRegression_477f927862dad5f58878', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.4,\n",
       "  Param(parent='LogisticRegression_477f927862dad5f58878', name='regParam', doc='regularization parameter (>= 0).'): 0.01},\n",
       " {Param(parent='LogisticRegression_477f927862dad5f58878', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.4,\n",
       "  Param(parent='LogisticRegression_477f927862dad5f58878', name='regParam', doc='regularization parameter (>= 0).'): 0.02}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model 1\n",
      "Fitting model 2\n",
      "Fitting model 3\n",
      "Fitting model 4\n",
      "Fitting model 5\n",
      "Fitting model 6\n",
      "Fitting model 7\n",
      "Fitting model 8\n",
      "Fitting model 9\n"
     ]
    }
   ],
   "source": [
    "all_models = []\n",
    "for j in range(len(grid)):\n",
    "    print(\"Fitting model {}\".format(j+1))\n",
    "    model = en_lr_estimator.fit(training_df, grid[j])\n",
    "    all_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9171142613953215"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estimate the accuracy of each of them:\n",
    "accuracies = [m.\\\n",
    "    transform(validation_df).\\\n",
    "    select(fn.avg(fn.expr('Score = prediction').astype('float')).alias('accuracy')).\\\n",
    "    first().\\\n",
    "    accuracy for m in all_models]\n",
    "import numpy as np\n",
    "best_model_idx = np.argmax(accuracies)\n",
    "best_model = all_models[best_model_idx]\n",
    "accuracies[best_model_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|avg(float((prediction = score)))|\n",
      "+--------------------------------+\n",
      "|              0.9181993343843055|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model.transform(testing_df).select(fn.avg(fn.expr('float(prediction = Score)'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "vocabulary = idf_pipeline.stages[0].stages[-1].vocabulary\n",
    "en_weights = best_model.stages[-1].coefficients.toArray()\n",
    "en_coeffs_df = pd.DataFrame({'word': vocabulary, 'weight': en_weights})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34276</th>\n",
       "      <td>-0.956534</td>\n",
       "      <td>rattlers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25679</th>\n",
       "      <td>-0.913749</td>\n",
       "      <td>ramada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31049</th>\n",
       "      <td>-0.842255</td>\n",
       "      <td>chatter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29920</th>\n",
       "      <td>-0.837219</td>\n",
       "      <td>coils</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34711</th>\n",
       "      <td>-0.762865</td>\n",
       "      <td>distrustful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         weight         word\n",
       "34276 -0.956534     rattlers\n",
       "25679 -0.913749       ramada\n",
       "31049 -0.842255      chatter\n",
       "29920 -0.837219        coils\n",
       "34711 -0.762865  distrustful"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Negative \n",
    "en_coeffs_df.sort_values('weight').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27243</th>\n",
       "      <td>1.056668</td>\n",
       "      <td>accuse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34110</th>\n",
       "      <td>1.001762</td>\n",
       "      <td>talcum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21628</th>\n",
       "      <td>0.835333</td>\n",
       "      <td>hiss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28287</th>\n",
       "      <td>0.802772</td>\n",
       "      <td>netrush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30953</th>\n",
       "      <td>0.782264</td>\n",
       "      <td>hiring</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         weight     word\n",
       "27243  1.056668   accuse\n",
       "34110  1.001762   talcum\n",
       "21628  0.835333     hiss\n",
       "28287  0.802772  netrush\n",
       "30953  0.782264   hiring"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Positive  \n",
    "en_coeffs_df.sort_values('weight', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---------+------------+-----+------------+--------------------+-------------------+\n",
      "| ID|    Time|IsWeekday|IsHolidayUSA|Score|     Summary|                Text|HelpfulnessFraction|\n",
      "+---+--------+---------+------------+-----+------------+--------------------+-------------------+\n",
      "| 94|10/31/02|        1|           1|    1|great comedy|beetlejuice is th...|                  1|\n",
      "+---+--------+---------+------------+-----+------------+--------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "notHolidays_df = fineFood_df.where(fn.col('IsHolidayUSA') == 1)\n",
    "notHolidays_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we are building models on the dataset in which all reviews were done in holiday "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_pipeline_h = Pipeline(stages=[tokenizer, sw_filter, cv]).fit(notHolidays_df)\n",
    "idf_pipeline_h = Pipeline(stages=[cv_pipeline_h, idf]).fit(notHolidays_df)\n",
    "training_df_h, validation_df_h, testing_df_h = notHolidays_df.randomSplit([0.6, 0.3, 0.1], seed=0)\n",
    "en_lr_estimator_h = Pipeline(stages=[idf_pipeline_h, en_lr])\n",
    "grid = ParamGridBuilder().\\\n",
    "    addGrid(en_lr.regParam, [0., 0.01, 0.02]).\\\n",
    "    addGrid(en_lr.elasticNetParam, [0., 0.2, 0.4]).\\\n",
    "    build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model 1\n",
      "Fitting model 2\n",
      "Fitting model 3\n",
      "Fitting model 4\n",
      "Fitting model 5\n",
      "Fitting model 6\n",
      "Fitting model 7\n",
      "Fitting model 8\n",
      "Fitting model 9\n"
     ]
    }
   ],
   "source": [
    "all_models_h = []\n",
    "for j in range(len(grid)):\n",
    "    print(\"Fitting model {}\".format(j+1))\n",
    "    model = en_lr_estimator_h.fit(training_df_h, grid[j])\n",
    "    all_models_h.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9171142613953215"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies_h = [m.\\\n",
    "    transform(validation_df).\\\n",
    "    select(fn.avg(fn.expr('Score = prediction').astype('float')).alias('accuracy')).\\\n",
    "    first().\\\n",
    "    accuracy for m in all_models_h]\n",
    "import numpy as np\n",
    "best_model_h_idx = np.argmax(accuracies)\n",
    "best_model_h = all_models[best_model_h_idx]\n",
    "accuracies_h[best_model_h_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|avg(float((prediction = Score)))|\n",
      "+--------------------------------+\n",
      "|              0.9343434343434344|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model_h.transform(testing_df_h).select(fn.avg(fn.expr('float(prediction = Score)'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9089665698101025,\n",
       " 0.9171142613953215,\n",
       " 0.9157641395908543,\n",
       " 0.9089665698101025,\n",
       " 0.897924921487482,\n",
       " 0.8847348184673183,\n",
       " 0.9089665698101025,\n",
       " 0.8863608347274808,\n",
       " 0.8743682310469314]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8481406474714567,\n",
       " 0.8679287370491033,\n",
       " 0.8695195327404538,\n",
       " 0.8481406474714567,\n",
       " 0.8684981362449004,\n",
       " 0.8700654515570426,\n",
       " 0.8481406474714567,\n",
       " 0.8695782336884741,\n",
       " 0.8686448886149511]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building models on not holidays dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "notHolidays_df = fineFood_df.where(fn.col('IsHolidayUSA') == 0)\n",
    "cv_pipeline_noth = Pipeline(stages=[tokenizer, sw_filter, cv]).fit(notHolidays_df)\n",
    "idf_pipeline_noth = Pipeline(stages=[cv_pipeline_h, idf]).fit(notHolidays_df)\n",
    "training_df_noth, validation_df_noth, testing_df_noth = notHolidays_df.randomSplit([0.6, 0.3, 0.1], seed=0)\n",
    "en_lr_estimator_noth = Pipeline(stages=[idf_pipeline_noth, en_lr])\n",
    "grid = ParamGridBuilder().\\\n",
    "    addGrid(en_lr.regParam, [0., 0.01, 0.02]).\\\n",
    "    addGrid(en_lr.elasticNetParam, [0., 0.2, 0.4]).\\\n",
    "    build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model 1\n",
      "Fitting model 2\n",
      "Fitting model 3\n",
      "Fitting model 4\n",
      "Fitting model 5\n",
      "Fitting model 6\n",
      "Fitting model 7\n",
      "Fitting model 8\n",
      "Fitting model 9\n"
     ]
    }
   ],
   "source": [
    "all_models_notholiday = []\n",
    "for j in range(len(grid)):\n",
    "    print(\"Fitting model {}\".format(j+1))\n",
    "    model = en_lr_estimator_noth.fit(training_df_noth, grid[j])\n",
    "    all_models_notholiday.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracies_noth = [m.\\\n",
    "    transform(validation_df).\\\n",
    "    select(fn.avg(fn.expr('Score = prediction').astype('float')).alias('accuracy')).\\\n",
    "    first().\\\n",
    "    accuracy for m in all_models_notholiday]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9115494115229961,\n",
       " 0.9093892166358487,\n",
       " 0.906800504828153,\n",
       " 0.9115494115229961,\n",
       " 0.8949663937072584,\n",
       " 0.8845587156232573,\n",
       " 0.9115494115229961,\n",
       " 0.8862903935898565,\n",
       " 0.8737929617563324]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies_noth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_477f927862dad5f58878', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,\n",
       " Param(parent='LogisticRegression_477f927862dad5f58878', name='regParam', doc='regularization parameter (>= 0).'): 0.01}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_noth_idx = np.argmax(accuracies_noth)\n",
    "best_model_noth = all_models[best_model_h_idx]\n",
    "grid[best_model_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_477f927862dad5f58878', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,\n",
       " Param(parent='LogisticRegression_477f927862dad5f58878', name='regParam', doc='regularization parameter (>= 0).'): 0.01}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid[best_model_h_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_477f927862dad5f58878', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,\n",
       " Param(parent='LogisticRegression_477f927862dad5f58878', name='regParam', doc='regularization parameter (>= 0).'): 0.0}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid[best_model_noth_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to apply SVM on this dataset but we could not fit the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idf_pipeline = Pipeline(stages=[cv_pipeline, idf]).fit(fineFood_df)\n",
    "svmFineFood_df= idf_pipeline.transform(fineFood_df).select('tf','tfidf','HelpfulnessFraction','Score','IsHolidayUSA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_training_df, svm_validation_df, svm_testing_df = svmFineFood_df.randomSplit([0.6, 0.3, 0.1], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-87e6bbe79233>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSVMWithSGD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSVMModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mSVMWithSGD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvm_training_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tfidf'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/mllib/classification.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(cls, data, iterations, step, regParam, miniBatchFraction, initialWeights, regType, intercept, validateData, convergenceTol)\u001b[0m\n\u001b[0;32m    551\u001b[0m                                  bool(intercept), bool(validateData), float(convergenceTol))\n\u001b[0;32m    552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_regression_train_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSVMModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitialWeights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/mllib/regression.py\u001b[0m in \u001b[0;36m_regression_train_wrapper\u001b[1;34m(train_func, modelClass, data, initial_weights)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_regression_train_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodelClass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m     \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLabeledPoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data should be an RDD of LabeledPoint, but got %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "model= SVMWithSGD.train(svm_training_df['tfidf'], iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
